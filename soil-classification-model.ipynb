{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11886402,"sourceType":"datasetVersion","datasetId":7470809}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\n\nfrom sklearn.model_selection import train_test_split\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ntry:\n    import albumentations, tqdm\nexcept ImportError:\n    !pip install --quiet albumentations tqdm\n    import albumentations, tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-05-21T08:46:28.905197Z","iopub.execute_input":"2025-05-21T08:46:28.905792Z","iopub.status.idle":"2025-05-21T08:46:33.151640Z","shell.execute_reply.started":"2025-05-21T08:46:28.905766Z","shell.execute_reply":"2025-05-21T08:46:33.150851Z"},"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n# 1. Load the CSV of image IDs and labels\nlabels = pd.read_csv(\n    '/kaggle/input/soil-classification-dataset-2025/train_labels.csv'\n)\n\n# 2. Add the full file path for each image\nlabels['file_path'] = (\n    '/kaggle/input/soil-classification-dataset-2025/train/'\n    + labels['image_id']\n)\n\n# 3. Prepare Stratified K-Fold splits (5 folds)\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfolds = []\nfor train_idx, val_idx in skf.split(labels, labels['soil_type']):\n    train_df_fold = labels.iloc[train_idx].reset_index(drop=True)\n    val_df_fold   = labels.iloc[val_idx].reset_index(drop=True)\n    folds.append((train_df_fold, val_df_fold))\n\nprint(f\"Found {len(labels)} images across {labels['soil_type'].nunique()} classes.\")\nprint(f\"Prepared {n_splits} stratified folds.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T08:46:36.747913Z","iopub.execute_input":"2025-05-21T08:46:36.748346Z","iopub.status.idle":"2025-05-21T08:46:36.765931Z","shell.execute_reply.started":"2025-05-21T08:46:36.748322Z","shell.execute_reply":"2025-05-21T08:46:36.765232Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Found 1222 images across 4 classes.\nPrepared 5 stratified folds.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Model, Optimizer, Scheduler, Loss\n\n# 0. Ensure RangerAdaBelief is installed\n!pip install --quiet ranger-adabelief\n\nimport torch\nfrom torchvision import models\nfrom ranger_adabelief import RangerAdaBelief\nfrom torch.optim.lr_scheduler import CyclicLR\n\n# 1. Device and model (using the new weights API)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nweights = models.ResNet50_Weights.DEFAULT\nmodel = models.resnet50(weights=weights).to(device)\n\n# 2. Optimizer with weight decay (regularization)\noptimizer = RangerAdaBelief(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=1e-3,\n    weight_decay=1e-4\n)\n\n# 3. Scheduler (Cyclical Learning Rate)\nscheduler = CyclicLR(\n    optimizer,\n    base_lr=1e-5,\n    max_lr=1e-3,\n    step_size_up=2000,\n    mode='triangular2',\n    cycle_momentum=False\n)\n\n# 4. Loss with label smoothing\ncriterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T08:46:39.895957Z","iopub.execute_input":"2025-05-21T08:46:39.896458Z","iopub.status.idle":"2025-05-21T08:46:43.544067Z","shell.execute_reply.started":"2025-05-21T08:46:39.896434Z","shell.execute_reply":"2025-05-21T08:46:43.543251Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Ranger optimizer loaded. \nGradient Centralization usage = True\nGC applied to both conv and fc layers\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 4: Transforms, Dataset, and DataLoaders\n\n# 1. Define Albumentations transforms\ntrain_transforms = A.Compose([\n    A.Resize(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=30, p=0.5),\n    A.RandomBrightnessContrast(p=0.5),\n    A.GaussianBlur(p=0.3),\n    A.CoarseDropout(\n        num_holes_range=(1, 8),\n        hole_height_range=(1, 16),\n        hole_width_range=(1, 16),\n        p=0.5\n    ),\n    A.Normalize(),\n    ToTensorV2()\n])\n\nval_transforms = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(),\n    ToTensorV2()\n])\n\n# 2. Custom Dataset class\nclass SoilDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n        self.label_map = {\n            'Alluvial soil': 0,\n            'Black Soil': 1,\n            'Clay soil': 2,\n            'Red soil': 3\n        }\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['file_path']).convert('RGB')\n        image = np.array(image)\n        if self.transforms:\n            image = self.transforms(image=image)['image']\n        label = self.label_map[row['soil_type']]\n        return image, label\n\n# 3. Select the first fold for a quick setup\ntrain_df, val_df = folds[0]\n\n# 4. Create datasets and dataloaders\ntrain_dataset = SoilDataset(train_df, transforms=train_transforms)\nval_dataset   = SoilDataset(val_df,   transforms=val_transforms)\n\n# Using num_workers=0 to avoid multiprocessing issues\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4,pin_memory=True)\nval_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4,pin_memory=True)\n\n# 5. Quick check\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Validation batches: {len(val_loader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:01:48.268107Z","iopub.execute_input":"2025-05-21T09:01:48.268823Z","iopub.status.idle":"2025-05-21T09:01:48.284905Z","shell.execute_reply.started":"2025-05-21T09:01:48.268791Z","shell.execute_reply":"2025-05-21T09:01:48.284059Z"}},"outputs":[{"name":"stdout","text":"Train batches: 31\nValidation batches: 8\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\ndef train_one_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    for images, labels in dataloader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        \n    epoch_loss = running_loss / len(dataloader.dataset)\n    precision = precision_score(all_labels, all_preds, average='weighted')\n    recall = recall_score(all_labels, all_preds, average='weighted')\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    print(f\"Train Loss: {epoch_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\ndef validate_one_epoch(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n    epoch_loss = running_loss / len(dataloader.dataset)\n    f1_per_class = f1_score(all_labels, all_preds, average=None)\n    for idx, score in enumerate(f1_per_class):\n        print(f\"F1 Score for class {idx}: {score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:01:55.206902Z","iopub.execute_input":"2025-05-21T09:01:55.207395Z","iopub.status.idle":"2025-05-21T09:01:55.215389Z","shell.execute_reply.started":"2025-05-21T09:01:55.207371Z","shell.execute_reply":"2025-05-21T09:01:55.214800Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from ranger_adabelief import RangerAdaBelief\nfrom torch.optim.lr_scheduler import CyclicLR\nimport gc\n\nnum_epochs = 55\nbest_fold_models = []\n\nfor fold, (train_df_fold, val_df_fold) in enumerate(folds, 1):\n    print(f\"\\n===== Fold {fold}/5 =====\")\n    \n    # Clear memory before starting a new fold\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Prepare loaders (using your successful configuration)\n    train_loader = DataLoader(\n        SoilDataset(train_df_fold, train_transforms),\n        batch_size=32, shuffle=True, num_workers=4, pin_memory=True\n    )\n    val_loader = DataLoader(\n        SoilDataset(val_df_fold, val_transforms),\n        batch_size=32, shuffle=False, num_workers=4, pin_memory=True\n    )\n    \n    # Model - use weights parameter but equivalent to pretrained=True\n    model_fold = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n    \n    # Freeze early layers (exactly as in your successful code)\n    for name, param in model_fold.named_parameters():\n        param.requires_grad = not (name.startswith('layer1') or name.startswith('layer2'))\n    \n    # Custom head with higher dropout for better generalization\n    in_features = model_fold.fc.in_features\n    model_fold.fc = torch.nn.Sequential(\n        torch.nn.Dropout(p=0.5),\n        torch.nn.Linear(in_features, 512),\n        torch.nn.ReLU(),\n        torch.nn.Dropout(p=0.3),\n        torch.nn.Linear(512, 4)\n    )\n    model_fold = model_fold.to(device)\n    \n    # Optimizer with slightly lower learning rate to prevent overfitting\n    optimizer_fold = RangerAdaBelief(\n        filter(lambda p: p.requires_grad, model_fold.parameters()),\n        lr=8e-4,  # Slightly lower than original\n        weight_decay=1e-4,\n        eps=1e-8,  # For numerical stability\n        betas=(0.9, 0.999)\n    )\n    \n    # Scheduler (same as your successful version)\n    scheduler = CyclicLR(\n        optimizer_fold,\n        base_lr=1e-5,\n        max_lr=1e-3,\n        step_size_up=2000,\n        mode='triangular2',\n        cycle_momentum=False\n    )\n    \n    best_min_f1 = 0.0\n    best_state = None\n    \n    for epoch in range(1, num_epochs + 1):\n        print(f\"Epoch {epoch}/{num_epochs}\")\n        \n        # Training\n        model_fold.train()\n        run_loss = 0\n        train_preds, train_labels = [], []\n        \n        for imgs, lbls in train_loader:\n            imgs, lbls = imgs.to(device), lbls.to(device)\n            optimizer_fold.zero_grad()\n            outputs = model_fold(imgs)\n            loss = criterion(outputs, lbls)\n            loss.backward()\n            \n            # Gradient clipping to prevent exploding gradients\n            torch.nn.utils.clip_grad_norm_(model_fold.parameters(), max_norm=1.0)\n            \n            optimizer_fold.step()\n            scheduler.step()  # Keep stepping on every batch as in original code\n            \n            run_loss += loss.item() * imgs.size(0)\n            train_preds.extend(outputs.argmax(1).cpu().numpy())\n            train_labels.extend(lbls.cpu().numpy())\n        \n        # Calculate training metrics once per epoch\n        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n        print(f\"  Train F1: {train_f1:.4f}\")\n        \n        # Validation\n        model_fold.eval()\n        val_preds, val_labels, val_loss = [], [], 0\n        \n        with torch.no_grad():\n            for imgs, lbls in val_loader:\n                imgs, lbls = imgs.to(device), lbls.to(device)\n                outputs = model_fold(imgs)\n                val_loss += criterion(outputs, lbls).item() * imgs.size(0)\n                val_preds.extend(outputs.argmax(1).cpu().numpy())\n                val_labels.extend(lbls.cpu().numpy())\n        \n        # Calculate F1 scores\n        f1_per_class = f1_score(val_labels, val_preds, average=None)\n        min_f1 = f1_per_class.min()\n        avg_f1 = f1_score(val_labels, val_preds, average='weighted')\n        \n        print(f\"  Fold {fold} Val Min F1: {min_f1:.4f}, Avg F1: {avg_f1:.4f}\")\n        \n        # Save best model based on minimum class F1 score\n        if min_f1 > best_min_f1:\n            best_min_f1 = min_f1\n            best_state = {k: v.cpu().detach().clone() for k, v in model_fold.state_dict().items()}\n            print(f\"  New best for fold {fold}: {best_min_f1:.4f}\")\n    \n    # Save best model\n    ckpt_path = f\"/kaggle/working/resnet50_fold{fold}_best.pth\"\n    torch.save(best_state, ckpt_path)\n    best_fold_models.append(ckpt_path)\n    print(f\"Saved fold {fold} model to {ckpt_path}\")\n    \n    # Clean up to prevent memory issues\n    del model_fold, optimizer_fold, scheduler, best_state\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:24:24.637873Z","iopub.execute_input":"2025-05-21T09:24:24.638196Z","iopub.status.idle":"2025-05-21T09:55:52.905278Z","shell.execute_reply.started":"2025-05-21T09:24:24.638170Z","shell.execute_reply":"2025-05-21T09:55:52.904640Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n===== Fold 1/5 =====\nRanger optimizer loaded. \nGradient Centralization usage = True\nGC applied to both conv and fc layers\nEpoch 1/55\n  Train F1: 0.2334\n  Fold 1 Val Min F1: 0.0000, Avg F1: 0.1978\nEpoch 2/55\n  Train F1: 0.2575\n  Fold 1 Val Min F1: 0.0351, Avg F1: 0.2326\n  New best for fold 1: 0.0351\nEpoch 3/55\n  Train F1: 0.2915\n  Fold 1 Val Min F1: 0.0000, Avg F1: 0.2576\nEpoch 4/55\n  Train F1: 0.2725\n  Fold 1 Val Min F1: 0.0000, Avg F1: 0.3119\nEpoch 5/55\n  Train F1: 0.2817\n  Fold 1 Val Min F1: 0.0000, Avg F1: 0.4474\nEpoch 6/55\n  Train F1: 0.3489\n  Fold 1 Val Min F1: 0.0000, Avg F1: 0.5249\nEpoch 7/55\n  Train F1: 0.4577\n  Fold 1 Val Min F1: 0.0930, Avg F1: 0.5971\n  New best for fold 1: 0.0930\nEpoch 8/55\n  Train F1: 0.5069\n  Fold 1 Val Min F1: 0.2174, Avg F1: 0.6537\n  New best for fold 1: 0.2174\nEpoch 9/55\n  Train F1: 0.5371\n  Fold 1 Val Min F1: 0.2553, Avg F1: 0.6831\n  New best for fold 1: 0.2553\nEpoch 10/55\n  Train F1: 0.5795\n  Fold 1 Val Min F1: 0.4151, Avg F1: 0.7263\n  New best for fold 1: 0.4151\nEpoch 11/55\n  Train F1: 0.6370\n  Fold 1 Val Min F1: 0.4561, Avg F1: 0.7404\n  New best for fold 1: 0.4561\nEpoch 12/55\n  Train F1: 0.7101\n  Fold 1 Val Min F1: 0.5333, Avg F1: 0.7732\n  New best for fold 1: 0.5333\nEpoch 13/55\n  Train F1: 0.7345\n  Fold 1 Val Min F1: 0.5574, Avg F1: 0.7949\n  New best for fold 1: 0.5574\nEpoch 14/55\n  Train F1: 0.8053\n  Fold 1 Val Min F1: 0.6154, Avg F1: 0.8024\n  New best for fold 1: 0.6154\nEpoch 15/55\n  Train F1: 0.8178\n  Fold 1 Val Min F1: 0.6364, Avg F1: 0.8339\n  New best for fold 1: 0.6364\nEpoch 16/55\n  Train F1: 0.8417\n  Fold 1 Val Min F1: 0.6866, Avg F1: 0.8520\n  New best for fold 1: 0.6866\nEpoch 17/55\n  Train F1: 0.8698\n  Fold 1 Val Min F1: 0.7429, Avg F1: 0.8734\n  New best for fold 1: 0.7429\nEpoch 18/55\n  Train F1: 0.8840\n  Fold 1 Val Min F1: 0.8000, Avg F1: 0.8918\n  New best for fold 1: 0.8000\nEpoch 19/55\n  Train F1: 0.8901\n  Fold 1 Val Min F1: 0.8421, Avg F1: 0.9046\n  New best for fold 1: 0.8421\nEpoch 20/55\n  Train F1: 0.9154\n  Fold 1 Val Min F1: 0.9000, Avg F1: 0.9546\n  New best for fold 1: 0.9000\nEpoch 21/55\n  Train F1: 0.9209\n  Fold 1 Val Min F1: 0.8916, Avg F1: 0.9347\nEpoch 22/55\n  Train F1: 0.9295\n  Fold 1 Val Min F1: 0.9286, Avg F1: 0.9674\n  New best for fold 1: 0.9286\nEpoch 23/55\n  Train F1: 0.9387\n  Fold 1 Val Min F1: 0.9157, Avg F1: 0.9633\nEpoch 24/55\n  Train F1: 0.9478\n  Fold 1 Val Min F1: 0.9286, Avg F1: 0.9674\nEpoch 25/55\n  Train F1: 0.9559\n  Fold 1 Val Min F1: 0.9412, Avg F1: 0.9716\n  New best for fold 1: 0.9412\nEpoch 26/55\n  Train F1: 0.9518\n  Fold 1 Val Min F1: 0.9398, Avg F1: 0.9672\nEpoch 27/55\n  Train F1: 0.9529\n  Fold 1 Val Min F1: 0.9412, Avg F1: 0.9674\nEpoch 28/55\n  Train F1: 0.9600\n  Fold 1 Val Min F1: 0.9286, Avg F1: 0.9676\nEpoch 29/55\n  Train F1: 0.9744\n  Fold 1 Val Min F1: 0.9286, Avg F1: 0.9590\nEpoch 30/55\n  Train F1: 0.9723\n  Fold 1 Val Min F1: 0.9302, Avg F1: 0.9629\nEpoch 31/55\n  Train F1: 0.9703\n  Fold 1 Val Min F1: 0.9398, Avg F1: 0.9672\nEpoch 32/55\n  Train F1: 0.9816\n  Fold 1 Val Min F1: 0.9302, Avg F1: 0.9629\nEpoch 33/55\n  Train F1: 0.9683\n  Fold 1 Val Min F1: 0.9412, Avg F1: 0.9716\nEpoch 34/55\n  Train F1: 0.9837\n  Fold 1 Val Min F1: 0.9630, Avg F1: 0.9755\n  New best for fold 1: 0.9630\nEpoch 35/55\n  Train F1: 0.9786\n  Fold 1 Val Min F1: 0.9750, Avg F1: 0.9837\n  New best for fold 1: 0.9750\nEpoch 36/55\n  Train F1: 0.9764\n  Fold 1 Val Min F1: 0.9545, Avg F1: 0.9713\nEpoch 37/55\n  Train F1: 0.9765\n  Fold 1 Val Min F1: 0.9500, Avg F1: 0.9755\nEpoch 38/55\n  Train F1: 0.9805\n  Fold 1 Val Min F1: 0.9630, Avg F1: 0.9796\nEpoch 39/55\n  Train F1: 0.9857\n  Fold 1 Val Min F1: 0.9630, Avg F1: 0.9796\nEpoch 40/55\n  Train F1: 0.9846\n  Fold 1 Val Min F1: 0.9630, Avg F1: 0.9796\nEpoch 41/55\n  Train F1: 0.9867\n  Fold 1 Val Min F1: 0.9630, Avg F1: 0.9796\nEpoch 42/55\n  Train F1: 0.9887\n  Fold 1 Val Min F1: 0.9630, Avg F1: 0.9755\nEpoch 43/55\n  Train F1: 0.9836\n  Fold 1 Val Min F1: 0.9630, Avg F1: 0.9755\nEpoch 44/55\n  Train F1: 0.9898\n  Fold 1 Val Min F1: 0.9630, Avg F1: 0.9755\nEpoch 45/55\n  Train F1: 0.9836\n  Fold 1 Val Min F1: 0.9512, Avg F1: 0.9715\nEpoch 46/55\n  Train F1: 0.9856\n  Fold 1 Val Min F1: 0.9663, Avg F1: 0.9836\nEpoch 47/55\n  Train F1: 0.9867\n  Fold 1 Val Min F1: 0.9545, Avg F1: 0.9753\nEpoch 48/55\n  Train F1: 0.9847\n  Fold 1 Val Min F1: 0.9750, Avg F1: 0.9837\nEpoch 49/55\n  Train F1: 0.9887\n  Fold 1 Val Min F1: 0.9877, Avg F1: 0.9918\n  New best for fold 1: 0.9877\nEpoch 50/55\n  Train F1: 0.9877\n  Fold 1 Val Min F1: 0.9663, Avg F1: 0.9795\nEpoch 51/55\n  Train F1: 0.9928\n  Fold 1 Val Min F1: 0.9512, Avg F1: 0.9673\nEpoch 52/55\n  Train F1: 0.9908\n  Fold 1 Val Min F1: 0.9512, Avg F1: 0.9715\nEpoch 53/55\n  Train F1: 0.9969\n  Fold 1 Val Min F1: 0.9750, Avg F1: 0.9836\nEpoch 54/55\n  Train F1: 0.9918\n  Fold 1 Val Min F1: 0.9512, Avg F1: 0.9756\nEpoch 55/55\n  Train F1: 0.9939\n  Fold 1 Val Min F1: 0.9512, Avg F1: 0.9756\nSaved fold 1 model to /kaggle/working/resnet50_fold1_best.pth\n\n===== Fold 2/5 =====\nRanger optimizer loaded. \nGradient Centralization usage = True\nGC applied to both conv and fc layers\nEpoch 1/55\n  Train F1: 0.2991\n  Fold 2 Val Min F1: 0.0476, Avg F1: 0.3763\n  New best for fold 2: 0.0476\nEpoch 2/55\n  Train F1: 0.3023\n  Fold 2 Val Min F1: 0.0339, Avg F1: 0.3286\nEpoch 3/55\n  Train F1: 0.3148\n  Fold 2 Val Min F1: 0.0465, Avg F1: 0.3587\nEpoch 4/55\n  Train F1: 0.3416\n  Fold 2 Val Min F1: 0.0952, Avg F1: 0.3787\n  New best for fold 2: 0.0952\nEpoch 5/55\n  Train F1: 0.3569\n  Fold 2 Val Min F1: 0.0727, Avg F1: 0.3988\nEpoch 6/55\n  Train F1: 0.3829\n  Fold 2 Val Min F1: 0.0727, Avg F1: 0.4373\nEpoch 7/55\n  Train F1: 0.4403\n  Fold 2 Val Min F1: 0.1379, Avg F1: 0.4884\n  New best for fold 2: 0.1379\nEpoch 8/55\n  Train F1: 0.4387\n  Fold 2 Val Min F1: 0.3333, Avg F1: 0.5657\n  New best for fold 2: 0.3333\nEpoch 9/55\n  Train F1: 0.4899\n  Fold 2 Val Min F1: 0.4528, Avg F1: 0.6501\n  New best for fold 2: 0.4528\nEpoch 10/55\n  Train F1: 0.5462\n  Fold 2 Val Min F1: 0.5263, Avg F1: 0.7153\n  New best for fold 2: 0.5263\nEpoch 11/55\n  Train F1: 0.6242\n  Fold 2 Val Min F1: 0.5172, Avg F1: 0.7550\nEpoch 12/55\n  Train F1: 0.6858\n  Fold 2 Val Min F1: 0.6970, Avg F1: 0.8326\n  New best for fold 2: 0.6970\nEpoch 13/55\n  Train F1: 0.7312\n  Fold 2 Val Min F1: 0.7778, Avg F1: 0.8718\n  New best for fold 2: 0.7778\nEpoch 14/55\n  Train F1: 0.7862\n  Fold 2 Val Min F1: 0.7887, Avg F1: 0.8876\n  New best for fold 2: 0.7887\nEpoch 15/55\n  Train F1: 0.8488\n  Fold 2 Val Min F1: 0.7945, Avg F1: 0.9002\n  New best for fold 2: 0.7945\nEpoch 16/55\n  Train F1: 0.8506\n  Fold 2 Val Min F1: 0.8533, Avg F1: 0.9336\n  New best for fold 2: 0.8533\nEpoch 17/55\n  Train F1: 0.8713\n  Fold 2 Val Min F1: 0.8571, Avg F1: 0.9381\n  New best for fold 2: 0.8571\nEpoch 18/55\n  Train F1: 0.8835\n  Fold 2 Val Min F1: 0.8462, Avg F1: 0.9383\nEpoch 19/55\n  Train F1: 0.8992\n  Fold 2 Val Min F1: 0.8500, Avg F1: 0.9304\nEpoch 20/55\n  Train F1: 0.8994\n  Fold 2 Val Min F1: 0.8608, Avg F1: 0.9467\n  New best for fold 2: 0.8608\nEpoch 21/55\n  Train F1: 0.9208\n  Fold 2 Val Min F1: 0.8718, Avg F1: 0.9507\n  New best for fold 2: 0.8718\nEpoch 22/55\n  Train F1: 0.9334\n  Fold 2 Val Min F1: 0.9000, Avg F1: 0.9592\n  New best for fold 2: 0.9000\nEpoch 23/55\n  Train F1: 0.9334\n  Fold 2 Val Min F1: 0.9268, Avg F1: 0.9634\n  New best for fold 2: 0.9268\nEpoch 24/55\n  Train F1: 0.9489\n  Fold 2 Val Min F1: 0.8718, Avg F1: 0.9548\nEpoch 25/55\n  Train F1: 0.9590\n  Fold 2 Val Min F1: 0.8718, Avg F1: 0.9548\nEpoch 26/55\n  Train F1: 0.9519\n  Fold 2 Val Min F1: 0.9268, Avg F1: 0.9716\nEpoch 27/55\n  Train F1: 0.9407\n  Fold 2 Val Min F1: 0.9367, Avg F1: 0.9754\n  New best for fold 2: 0.9367\nEpoch 28/55\n  Train F1: 0.9682\n  Fold 2 Val Min F1: 0.9500, Avg F1: 0.9796\n  New best for fold 2: 0.9500\nEpoch 29/55\n  Train F1: 0.9643\n  Fold 2 Val Min F1: 0.9620, Avg F1: 0.9877\n  New best for fold 2: 0.9620\nEpoch 30/55\n  Train F1: 0.9631\n  Fold 2 Val Min F1: 0.9620, Avg F1: 0.9877\nEpoch 31/55\n  Train F1: 0.9694\n  Fold 2 Val Min F1: 0.9268, Avg F1: 0.9757\nEpoch 32/55\n  Train F1: 0.9732\n  Fold 2 Val Min F1: 0.9383, Avg F1: 0.9797\nEpoch 33/55\n  Train F1: 0.9764\n  Fold 2 Val Min F1: 0.9620, Avg F1: 0.9877\nEpoch 34/55\n  Train F1: 0.9755\n  Fold 2 Val Min F1: 0.9620, Avg F1: 0.9877\nEpoch 35/55\n  Train F1: 0.9754\n  Fold 2 Val Min F1: 0.9351, Avg F1: 0.9793\nEpoch 36/55\n  Train F1: 0.9744\n  Fold 2 Val Min F1: 0.9620, Avg F1: 0.9877\nEpoch 37/55\n  Train F1: 0.9795\n  Fold 2 Val Min F1: 0.9500, Avg F1: 0.9837\nEpoch 38/55\n  Train F1: 0.9765\n  Fold 2 Val Min F1: 0.9500, Avg F1: 0.9837\nEpoch 39/55\n  Train F1: 0.9765\n  Fold 2 Val Min F1: 0.9620, Avg F1: 0.9877\nEpoch 40/55\n  Train F1: 0.9847\n  Fold 2 Val Min F1: 0.9620, Avg F1: 0.9877\nEpoch 41/55\n  Train F1: 0.9836\n  Fold 2 Val Min F1: 0.9500, Avg F1: 0.9837\nEpoch 42/55\n  Train F1: 0.9846\n  Fold 2 Val Min F1: 0.9620, Avg F1: 0.9877\nEpoch 43/55\n  Train F1: 0.9888\n  Fold 2 Val Min F1: 0.9500, Avg F1: 0.9837\nEpoch 44/55\n  Train F1: 0.9897\n  Fold 2 Val Min F1: 0.9620, Avg F1: 0.9877\nEpoch 45/55\n  Train F1: 0.9857\n  Fold 2 Val Min F1: 0.9500, Avg F1: 0.9837\nEpoch 46/55\n  Train F1: 0.9877\n  Fold 2 Val Min F1: 0.9873, Avg F1: 0.9959\n  New best for fold 2: 0.9873\nEpoch 47/55\n  Train F1: 0.9836\n  Fold 2 Val Min F1: 1.0000, Avg F1: 1.0000\n  New best for fold 2: 1.0000\nEpoch 48/55\n  Train F1: 0.9805\n  Fold 2 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 49/55\n  Train F1: 0.9846\n  Fold 2 Val Min F1: 0.9744, Avg F1: 0.9918\nEpoch 50/55\n  Train F1: 0.9959\n  Fold 2 Val Min F1: 0.9744, Avg F1: 0.9918\nEpoch 51/55\n  Train F1: 0.9928\n  Fold 2 Val Min F1: 0.9744, Avg F1: 0.9918\nEpoch 52/55\n  Train F1: 0.9887\n  Fold 2 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 53/55\n  Train F1: 0.9918\n  Fold 2 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 54/55\n  Train F1: 0.9898\n  Fold 2 Val Min F1: 0.9744, Avg F1: 0.9918\nEpoch 55/55\n  Train F1: 0.9918\n  Fold 2 Val Min F1: 0.9744, Avg F1: 0.9918\nSaved fold 2 model to /kaggle/working/resnet50_fold2_best.pth\n\n===== Fold 3/5 =====\nRanger optimizer loaded. \nGradient Centralization usage = True\nGC applied to both conv and fc layers\nEpoch 1/55\n  Train F1: 0.3124\n  Fold 3 Val Min F1: 0.0000, Avg F1: 0.3041\nEpoch 2/55\n  Train F1: 0.3149\n  Fold 3 Val Min F1: 0.0000, Avg F1: 0.2940\nEpoch 3/55\n  Train F1: 0.3054\n  Fold 3 Val Min F1: 0.0000, Avg F1: 0.3727\nEpoch 4/55\n  Train F1: 0.3202\n  Fold 3 Val Min F1: 0.0000, Avg F1: 0.4119\nEpoch 5/55\n  Train F1: 0.3827\n  Fold 3 Val Min F1: 0.0952, Avg F1: 0.4877\n  New best for fold 3: 0.0952\nEpoch 6/55\n  Train F1: 0.4188\n  Fold 3 Val Min F1: 0.1860, Avg F1: 0.5266\n  New best for fold 3: 0.1860\nEpoch 7/55\n  Train F1: 0.4661\n  Fold 3 Val Min F1: 0.3750, Avg F1: 0.6420\n  New best for fold 3: 0.3750\nEpoch 8/55\n  Train F1: 0.4827\n  Fold 3 Val Min F1: 0.4082, Avg F1: 0.6501\n  New best for fold 3: 0.4082\nEpoch 9/55\n  Train F1: 0.5477\n  Fold 3 Val Min F1: 0.5000, Avg F1: 0.7155\n  New best for fold 3: 0.5000\nEpoch 10/55\n  Train F1: 0.5915\n  Fold 3 Val Min F1: 0.5818, Avg F1: 0.7933\n  New best for fold 3: 0.5818\nEpoch 11/55\n  Train F1: 0.6642\n  Fold 3 Val Min F1: 0.7000, Avg F1: 0.8626\n  New best for fold 3: 0.7000\nEpoch 12/55\n  Train F1: 0.7213\n  Fold 3 Val Min F1: 0.7097, Avg F1: 0.8398\n  New best for fold 3: 0.7097\nEpoch 13/55\n  Train F1: 0.7711\n  Fold 3 Val Min F1: 0.7692, Avg F1: 0.9022\n  New best for fold 3: 0.7692\nEpoch 14/55\n  Train F1: 0.7991\n  Fold 3 Val Min F1: 0.7879, Avg F1: 0.9148\n  New best for fold 3: 0.7879\nEpoch 15/55\n  Train F1: 0.8160\n  Fold 3 Val Min F1: 0.8116, Avg F1: 0.9200\n  New best for fold 3: 0.8116\nEpoch 16/55\n  Train F1: 0.8557\n  Fold 3 Val Min F1: 0.8286, Avg F1: 0.9286\n  New best for fold 3: 0.8286\nEpoch 17/55\n  Train F1: 0.8842\n  Fold 3 Val Min F1: 0.8378, Avg F1: 0.9337\n  New best for fold 3: 0.8378\nEpoch 18/55\n  Fold 3 Val Min F1: 0.9136, Avg F1: 0.9716\nEpoch 28/55\n  Train F1: 0.9632\n  Fold 3 Val Min F1: 0.9136, Avg F1: 0.9716\nEpoch 29/55\n  Train F1: 0.9704\n  Fold 3 Val Min F1: 0.9136, Avg F1: 0.9675\nEpoch 30/55\n  Train F1: 0.9653\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9674\nEpoch 31/55\n  Train F1: 0.9663\n  Fold 3 Val Min F1: 0.8916, Avg F1: 0.9555\nEpoch 32/55\n  Train F1: 0.9755\n  Fold 3 Val Min F1: 0.9500, Avg F1: 0.9796\n  New best for fold 3: 0.9500\nEpoch 33/55\n  Train F1: 0.9867\n  Fold 3 Val Min F1: 0.9620, Avg F1: 0.9836\n  New best for fold 3: 0.9620\nEpoch 34/55\n  Train F1: 0.9775\n  Fold 3 Val Min F1: 0.9500, Avg F1: 0.9796\nEpoch 35/55\n  Train F1: 0.9806\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9755\nEpoch 36/55\n  Train F1: 0.9816\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9714\nEpoch 37/55\n  Train F1: 0.9826\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9755\nEpoch 38/55\n  Train F1: 0.9806\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9714\nEpoch 39/55\n  Train F1: 0.9806\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9715\nEpoch 40/55\n  Train F1: 0.9857\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9674\nEpoch 41/55\n  Train F1: 0.9816\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9674\nEpoch 42/55\n  Train F1: 0.9795\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9673\nEpoch 43/55\n  Train F1: 0.9877\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9674\nEpoch 44/55\n  Train F1: 0.9898\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9714\nEpoch 45/55\n  Train F1: 0.9918\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9755\nEpoch 46/55\n  Train F1: 0.9826\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9715\nEpoch 47/55\n  Train F1: 0.9908\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9715\nEpoch 48/55\n  Train F1: 0.9867\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9633\nEpoch 49/55\n  Train F1: 0.9888\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9673\nEpoch 50/55\n  Train F1: 0.9898\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9714\nEpoch 51/55\n  Train F1: 0.9908\n  Fold 3 Val Min F1: 0.9500, Avg F1: 0.9755\nEpoch 52/55\n  Train F1: 0.9928\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9674\nEpoch 53/55\n  Train F1: 0.9867\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9755\nEpoch 54/55\n  Train F1: 0.9795\n  Fold 3 Val Min F1: 0.9367, Avg F1: 0.9714\nEpoch 55/55\n  Train F1: 0.9898\n  Fold 3 Val Min F1: 0.9250, Avg F1: 0.9715\nSaved fold 3 model to /kaggle/working/resnet50_fold3_best.pth\n\n===== Fold 4/5 =====\nRanger optimizer loaded. \nGradient Centralization usage = True\nGC applied to both conv and fc layers\nEpoch 1/55\n  Train F1: 0.2180\n  Fold 4 Val Min F1: 0.0000, Avg F1: 0.0803\nEpoch 2/55\n  Train F1: 0.2422\n  Fold 4 Val Min F1: 0.0000, Avg F1: 0.0833\nEpoch 3/55\n  Train F1: 0.2653\n  Fold 4 Val Min F1: 0.0000, Avg F1: 0.1609\nEpoch 4/55\n  Train F1: 0.2666\n  Fold 4 Val Min F1: 0.0000, Avg F1: 0.2774\nEpoch 5/55\n  Train F1: 0.3024\n  Fold 4 Val Min F1: 0.0000, Avg F1: 0.4239\nEpoch 6/55\n  Train F1: 0.3390\n  Fold 4 Val Min F1: 0.0000, Avg F1: 0.4695\nEpoch 7/55\n  Train F1: 0.3812\n  Fold 4 Val Min F1: 0.0000, Avg F1: 0.5280\nEpoch 8/55\n  Train F1: 0.4590\n  Fold 4 Val Min F1: 0.0000, Avg F1: 0.5756\nEpoch 9/55\n  Train F1: 0.4807\n  Fold 4 Val Min F1: 0.0488, Avg F1: 0.6202\n  New best for fold 4: 0.0488\nEpoch 10/55\n  Train F1: 0.5255\n  Fold 4 Val Min F1: 0.2979, Avg F1: 0.7027\n  New best for fold 4: 0.2979\nEpoch 11/55\n  Train F1: 0.6082\n  Fold 4 Val Min F1: 0.5185, Avg F1: 0.7773\n  New best for fold 4: 0.5185\nEpoch 12/55\n  Train F1: 0.6507\n  Fold 4 Val Min F1: 0.6333, Avg F1: 0.8267\n  New best for fold 4: 0.6333\nEpoch 13/55\n  Train F1: 0.7019\n  Fold 4 Val Min F1: 0.6557, Avg F1: 0.8366\n  New best for fold 4: 0.6557\nEpoch 14/55\n  Train F1: 0.7851\n  Fold 4 Val Min F1: 0.6774, Avg F1: 0.8459\n  New best for fold 4: 0.6774\nEpoch 15/55\n  Train F1: 0.8096\n  Fold 4 Val Min F1: 0.8116, Avg F1: 0.8946\n  New best for fold 4: 0.8116\nEpoch 16/55\n  Train F1: 0.8466\n  Fold 4 Val Min F1: 0.8219, Avg F1: 0.9080\n  New best for fold 4: 0.8219\nEpoch 17/55\n  Train F1: 0.8762\n  Fold 4 Val Min F1: 0.8706, Avg F1: 0.9124\n  New best for fold 4: 0.8706\nEpoch 18/55\n  Train F1: 0.8861\n  Fold 4 Val Min F1: 0.8684, Avg F1: 0.9250\nEpoch 19/55\n  Train F1: 0.9100\n  Fold 4 Val Min F1: 0.9067, Avg F1: 0.9379\n  New best for fold 4: 0.9067\nEpoch 20/55\n  Train F1: 0.9022\n  Fold 4 Val Min F1: 0.9091, Avg F1: 0.9504\n  New best for fold 4: 0.9091\nEpoch 21/55\n  Train F1: 0.9300\n  Fold 4 Val Min F1: 0.9213, Avg F1: 0.9505\n  New best for fold 4: 0.9213\nEpoch 22/55\n  Train F1: 0.9308\n  Fold 4 Val Min F1: 0.9487, Avg F1: 0.9713\n  New best for fold 4: 0.9487\nEpoch 23/55\n  Train F1: 0.9436\n  Fold 4 Val Min F1: 0.9211, Avg F1: 0.9629\nEpoch 24/55\n  Train F1: 0.9487\n  Fold 4 Val Min F1: 0.9487, Avg F1: 0.9713\nEpoch 25/55\n  Train F1: 0.9397\n  Fold 4 Val Min F1: 0.9462, Avg F1: 0.9672\nEpoch 26/55\n  Train F1: 0.9448\n  Fold 4 Val Min F1: 0.9367, Avg F1: 0.9712\nEpoch 27/55\n  Train F1: 0.9538\n  Fold 4 Val Min F1: 0.9487, Avg F1: 0.9753\nEpoch 28/55\n  Train F1: 0.9633\n  Fold 4 Val Min F1: 0.9487, Avg F1: 0.9753\nEpoch 29/55\n  Train F1: 0.9662\n  Fold 4 Val Min F1: 0.9474, Avg F1: 0.9713\nEpoch 30/55\n  Train F1: 0.9642\n  Fold 4 Val Min F1: 0.9474, Avg F1: 0.9755\nEpoch 31/55\n  Train F1: 0.9683\n  Fold 4 Val Min F1: 0.9474, Avg F1: 0.9755\nEpoch 32/55\n  Train F1: 0.9703\n  Fold 4 Val Min F1: 0.9474, Avg F1: 0.9755\nEpoch 33/55\n  Train F1: 0.9796\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9796\n  New best for fold 4: 0.9583\nEpoch 34/55\n  Train F1: 0.9785\n  Fold 4 Val Min F1: 0.9474, Avg F1: 0.9755\nEpoch 35/55\n  Train F1: 0.9713\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 36/55\n  Train F1: 0.9785\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 37/55\n  Train F1: 0.9826\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9796\nEpoch 38/55\n  Train F1: 0.9775\n  Fold 4 Val Min F1: 0.9333, Avg F1: 0.9710\nEpoch 39/55\n  Train F1: 0.9857\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 40/55\n  Train F1: 0.9847\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 41/55\n  Train F1: 0.9846\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 42/55\n  Train F1: 0.9918\n  Fold 4 Val Min F1: 0.9383, Avg F1: 0.9756\nEpoch 43/55\n  Train F1: 0.9866\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 44/55\n  Train F1: 0.9928\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 45/55\n  Train F1: 0.9908\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 46/55\n  Train F1: 0.9857\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 47/55\n  Train F1: 0.9908\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 48/55\n  Train F1: 0.9857\n  Fold 4 Val Min F1: 0.9787, Avg F1: 0.9919\n  New best for fold 4: 0.9787\nEpoch 49/55\n  Train F1: 0.9898\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 50/55\n  Train F1: 0.9877\n  Fold 4 Val Min F1: 0.9500, Avg F1: 0.9756\nEpoch 51/55\n  Train F1: 0.9877\n  Fold 4 Val Min F1: 0.9474, Avg F1: 0.9753\nEpoch 52/55\n  Train F1: 0.9877\n  Fold 4 Val Min F1: 0.9500, Avg F1: 0.9756\nEpoch 53/55\n  Train F1: 0.9888\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 54/55\n  Train F1: 0.9918\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nEpoch 55/55\n  Train F1: 0.9939\n  Fold 4 Val Min F1: 0.9583, Avg F1: 0.9838\nSaved fold 4 model to /kaggle/working/resnet50_fold4_best.pth\n\n===== Fold 5/5 =====\nRanger optimizer loaded. \nGradient Centralization usage = True\nGC applied to both conv and fc layers\nEpoch 1/55\n  Train F1: 0.2240\n  Fold 5 Val Min F1: 0.0308, Avg F1: 0.2760\n  New best for fold 5: 0.0308\nEpoch 2/55\n  Train F1: 0.2505\n  Fold 5 Val Min F1: 0.0333, Avg F1: 0.2878\n  New best for fold 5: 0.0333\nEpoch 3/55\n  Train F1: 0.2507\n  Fold 5 Val Min F1: 0.0333, Avg F1: 0.3571\nEpoch 4/55\n  Train F1: 0.2924\n  Fold 5 Val Min F1: 0.0408, Avg F1: 0.4460\n  New best for fold 5: 0.0408\nEpoch 5/55\n  Train F1: 0.3194\n  Fold 5 Val Min F1: 0.0417, Avg F1: 0.4901\n  New best for fold 5: 0.0417\nEpoch 6/55\n  Train F1: 0.3703\n  Fold 5 Val Min F1: 0.0455, Avg F1: 0.5221\n  New best for fold 5: 0.0455\nEpoch 7/55\n  Train F1: 0.4176\n  Fold 5 Val Min F1: 0.1364, Avg F1: 0.5727\n  New best for fold 5: 0.1364\nEpoch 8/55\n  Train F1: 0.4835\n  Fold 5 Val Min F1: 0.1739, Avg F1: 0.6329\n  New best for fold 5: 0.1739\nEpoch 9/55\n  Train F1: 0.5041\n  Fold 5 Val Min F1: 0.2500, Avg F1: 0.6984\n  New best for fold 5: 0.2500\nEpoch 10/55\n  Train F1: 0.5914\n  Fold 5 Val Min F1: 0.3846, Avg F1: 0.7496\n  New best for fold 5: 0.3846\nEpoch 11/55\n  Train F1: 0.6495\n  Fold 5 Val Min F1: 0.5862, Avg F1: 0.8003\n  New best for fold 5: 0.5862\nEpoch 12/55\n  Train F1: 0.6955\n  Fold 5 Val Min F1: 0.6774, Avg F1: 0.8335\n  New best for fold 5: 0.6774\nEpoch 13/55\n  Train F1: 0.7480\n  Fold 5 Val Min F1: 0.6984, Avg F1: 0.8470\n  New best for fold 5: 0.6984\nEpoch 14/55\n  Train F1: 0.7936\n  Fold 5 Val Min F1: 0.7273, Avg F1: 0.8734\n  New best for fold 5: 0.7273\nEpoch 15/55\n  Train F1: 0.8112\n  Fold 5 Val Min F1: 0.8333, Avg F1: 0.9052\n  New best for fold 5: 0.8333\nEpoch 16/55\n  Train F1: 0.8498\n  Fold 5 Val Min F1: 0.8800, Avg F1: 0.9299\n  New best for fold 5: 0.8800\nEpoch 17/55\n  Train F1: 0.8897\n  Fold 5 Val Min F1: 0.9091, Avg F1: 0.9463\n  New best for fold 5: 0.9091\nEpoch 18/55\n  Train F1: 0.8873\n  Fold 5 Val Min F1: 0.9231, Avg F1: 0.9505\n  New best for fold 5: 0.9231\nEpoch 19/55\n  Train F1: 0.8990\n  Fold 5 Val Min F1: 0.9231, Avg F1: 0.9547\nEpoch 20/55\n  Train F1: 0.9107\n  Fold 5 Val Min F1: 0.9367, Avg F1: 0.9588\n  New best for fold 5: 0.9367\nEpoch 21/55\n  Train F1: 0.9045\n  Fold 5 Val Min F1: 0.9500, Avg F1: 0.9630\n  New best for fold 5: 0.9500\nEpoch 22/55\n  Train F1: 0.9285\n  Fold 5 Val Min F1: 0.9500, Avg F1: 0.9630\nEpoch 23/55\n  Train F1: 0.9385\n  Fold 5 Val Min F1: 0.9630, Avg F1: 0.9836\n  New best for fold 5: 0.9630\nEpoch 24/55\n  Train F1: 0.9398\n  Fold 5 Val Min F1: 0.9630, Avg F1: 0.9836\nEpoch 25/55\n  Train F1: 0.9367\n  Fold 5 Val Min F1: 0.9750, Avg F1: 0.9877\n  New best for fold 5: 0.9750\nEpoch 26/55\n  Train F1: 0.9436\n  Fold 5 Val Min F1: 0.9750, Avg F1: 0.9877\nEpoch 27/55\n  Train F1: 0.9458\n  Fold 5 Val Min F1: 0.9750, Avg F1: 0.9877\nEpoch 28/55\n  Train F1: 0.9322\n  Fold 5 Val Min F1: 0.9750, Avg F1: 0.9877\nEpoch 29/55\n  Train F1: 0.9724\n  Fold 5 Val Min F1: 0.9750, Avg F1: 0.9877\nEpoch 30/55\n  Train F1: 0.9662\n  Fold 5 Val Min F1: 0.9877, Avg F1: 0.9918\n  New best for fold 5: 0.9877\nEpoch 31/55\n  Train F1: 0.9693\n  Fold 5 Val Min F1: 0.9877, Avg F1: 0.9918\nEpoch 32/55\n  Train F1: 0.9734\n  Fold 5 Val Min F1: 0.9890, Avg F1: 0.9959\n  New best for fold 5: 0.9890\nEpoch 33/55\n  Train F1: 0.9733\n  Fold 5 Val Min F1: 0.9890, Avg F1: 0.9959\nEpoch 34/55\n  Train F1: 0.9735\n  Fold 5 Val Min F1: 0.9873, Avg F1: 0.9918\nEpoch 35/55\n  Train F1: 0.9723\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\n  New best for fold 5: 1.0000\nEpoch 36/55\n  Train F1: 0.9836\n  Fold 5 Val Min F1: 0.9890, Avg F1: 0.9959\nEpoch 37/55\n  Train F1: 0.9857\n  Fold 5 Val Min F1: 0.9890, Avg F1: 0.9959\nEpoch 38/55\n  Train F1: 0.9805\n  Fold 5 Val Min F1: 0.9877, Avg F1: 0.9918\nEpoch 39/55\n  Train F1: 0.9836\n  Fold 5 Val Min F1: 0.9778, Avg F1: 0.9918\nEpoch 40/55\n  Train F1: 0.9785\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 41/55\n  Train F1: 0.9806\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 42/55\n  Train F1: 0.9796\n  Fold 5 Val Min F1: 0.9873, Avg F1: 0.9959\nEpoch 43/55\n  Train F1: 0.9867\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 44/55\n  Train F1: 0.9816\n  Fold 5 Val Min F1: 0.9877, Avg F1: 0.9918\nEpoch 45/55\n  Train F1: 0.9806\n  Fold 5 Val Min F1: 0.9890, Avg F1: 0.9959\nEpoch 46/55\n  Train F1: 0.9836\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 47/55\n  Train F1: 0.9898\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 48/55\n  Train F1: 0.9887\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 49/55\n  Train F1: 0.9856\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 50/55\n  Train F1: 0.9928\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 51/55\n  Train F1: 0.9908\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 52/55\n  Train F1: 0.9908\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 53/55\n  Train F1: 0.9908\n  Fold 5 Val Min F1: 0.9890, Avg F1: 0.9959\nEpoch 54/55\n  Train F1: 0.9867\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nEpoch 55/55\n  Train F1: 0.9898\n  Fold 5 Val Min F1: 1.0000, Avg F1: 1.0000\nSaved fold 5 model to /kaggle/working/resnet50_fold5_best.pth\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 7: Professional Ensemble Inference with F1 Score Evaluation\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, classification_report\n\n# Define TEST_DIR\nTEST_DIR = '/kaggle/input/soil-classification-dataset-2025/test'\nclasses = ['Alluvial soil', 'Black Soil', 'Clay soil', 'Red soil']\n\n# Test-time augmentations (TTA)\ntta_transforms = [\n    A.Compose([A.Resize(224, 224), A.Normalize(), ToTensorV2()]),\n    A.Compose([A.Resize(224, 224), A.HorizontalFlip(p=1.0), A.Normalize(), ToTensorV2()]),\n    A.Compose([A.Resize(256, 256), A.CenterCrop(224, 224), A.Normalize(), ToTensorV2()]),\n    A.Compose([A.Resize(224, 224), A.RandomBrightnessContrast(p=1.0), A.Normalize(), ToTensorV2()])\n]\n\n# Load models with correct architecture\nensemble = []\nfor path in best_fold_models:\n    m = models.resnet50(weights=None)\n    # Match the exact architecture used in training\n    in_features = m.fc.in_features\n    m.fc = torch.nn.Sequential(\n        torch.nn.Dropout(p=0.5),\n        torch.nn.Linear(in_features, 512),\n        torch.nn.ReLU(),\n        torch.nn.Dropout(p=0.3),\n        torch.nn.Linear(512, 4)\n    )\n    m.load_state_dict(torch.load(path))\n    m = m.to(device).eval()\n    ensemble.append(m)\n\nprint(f\"Loaded {len(ensemble)} models for ensemble prediction\")\n\n# Batch processing for efficiency\nbatch_size = 8\ntest_files = sorted(os.listdir(TEST_DIR))\nall_preds = []\n\n# Process in batches with progress bar\nfor i in tqdm(range(0, len(test_files), batch_size), desc=\"Predicting\"):\n    batch_files = test_files[i:i+batch_size]\n    batch_preds = []\n    \n    for fname in batch_files:\n        img_path = os.path.join(TEST_DIR, fname)\n        img = np.array(Image.open(img_path).convert('RGB'))\n        \n        # Apply Test-Time Augmentation (TTA)\n        tta_outputs = []\n        for transform in tta_transforms:\n            img_t = transform(image=img)['image'].unsqueeze(0).to(device)\n            \n            # Ensemble predictions\n            model_outputs = []\n            for model in ensemble:\n                with torch.no_grad():\n                    logits = model(img_t)\n                    scaled_logits = logits / 1.5  # Temperature scaling\n                    model_outputs.append(F.softmax(scaled_logits, dim=1))\n            \n            # Average the predictions from all models for this augmentation\n            avg_output = torch.stack(model_outputs).mean(0)\n            tta_outputs.append(avg_output)\n        \n        # Average predictions across all augmentations\n        final_output = torch.stack(tta_outputs).mean(0)\n        pred_class = classes[final_output.argmax(1).item()]\n        \n        batch_preds.append((fname, pred_class))\n    \n    all_preds.extend(batch_preds)\n\n# Create submission\nsubmission = pd.DataFrame(all_preds, columns=['image_id', 'soil_type'])\n\n# Save submission\nsubmission.to_csv('submission.csv', index=False)\nprint(f\"Saved submission.csv with {len(submission)} predictions\")\n\n# Calculate F1 scores on validation data\nprint(\"\\nEvaluating ensemble F1 scores on validation data...\")\n\n# Create a combined validation dataset from all folds\nval_images = []\nval_labels = []\n\nfor _, val_df_fold in folds:\n    val_dataset = SoilDataset(val_df_fold, val_transforms)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n    \n    # Collect validation data\n    for images, labels in val_loader:\n        val_images.append(images)\n        val_labels.append(labels)\n\n# Concatenate all validation data\nval_images = torch.cat(val_images)\nval_labels = torch.cat(val_labels).numpy()\n\n# Get ensemble predictions\nensemble_preds = []\nwith torch.no_grad():\n    for i in range(0, len(val_images), 32):\n        batch = val_images[i:i+32].to(device)\n        \n        # Aggregate predictions from all models\n        batch_preds = []\n        for model in ensemble:\n            outputs = model(batch)\n            batch_preds.append(torch.softmax(outputs, dim=1))\n        \n        # Average predictions from all models\n        avg_pred = torch.stack(batch_preds).mean(0)\n        predicted_classes = avg_pred.argmax(dim=1).cpu().numpy()\n        ensemble_preds.extend(predicted_classes)\n\n# Calculate and print F1 scores for each class\nf1_per_class = f1_score(val_labels, ensemble_preds, average=None)\n\nprint(\"\\nEnsemble F1 Scores by Class:\")\nfor i, class_name in enumerate(classes):\n    print(f\"{class_name}: {f1_per_class[i]:.4f}\")\n\n# Also print overall weighted F1 score\nweighted_f1 = f1_score(val_labels, ensemble_preds, average='weighted')\nprint(f\"\\nWeighted F1 Score: {weighted_f1:.4f}\")\n\n# Print detailed classification report\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(val_labels, ensemble_preds, target_names=classes))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:02:08.328236Z","iopub.execute_input":"2025-05-21T10:02:08.328803Z","iopub.status.idle":"2025-05-21T10:03:12.703163Z","shell.execute_reply.started":"2025-05-21T10:02:08.328772Z","shell.execute_reply":"2025-05-21T10:03:12.702501Z"}},"outputs":[{"name":"stdout","text":"Loaded 5 models for ensemble prediction\n","output_type":"stream"},{"name":"stderr","text":"Predicting: 100%|██████████| 43/43 [00:43<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Saved submission.csv with 341 predictions\n\nEvaluating ensemble F1 scores on validation data...\n\nEnsemble F1 Scores by Class:\nAlluvial soil: 0.9991\nBlack Soil: 0.9978\nClay soil: 1.0000\nRed soil: 1.0000\n\nWeighted F1 Score: 0.9992\n\nDetailed Classification Report:\n               precision    recall  f1-score   support\n\nAlluvial soil       1.00      1.00      1.00       528\n   Black Soil       1.00      1.00      1.00       231\n    Clay soil       1.00      1.00      1.00       199\n     Red soil       1.00      1.00      1.00       264\n\n     accuracy                           1.00      1222\n    macro avg       1.00      1.00      1.00      1222\n weighted avg       1.00      1.00      1.00      1222\n\n","output_type":"stream"}],"execution_count":19}]}